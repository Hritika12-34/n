{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cea086e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  base_word added_morph   new_word deleted_morph new_word(after_deletion)\n",
      "0      play         -ed     played             -                     play\n",
      "1       run        -ing    running             -                      run\n",
      "2     write         -er     writer             -                    write\n",
      "3     happy       -ness  happiness             -                    happy\n",
      "  base_word added_morph   new_word deleted_morph new_word(after_deletion)  \\\n",
      "0      play         -ed     played             -                     play   \n",
      "1       run        -ing    running             -                      run   \n",
      "2     write         -er     writer             -                    write   \n",
      "3     happy       -ness  happiness             -                    happy   \n",
      "\n",
      "  New Word [After Addition] New Word [After Deletion]  \n",
      "0                   play-ed                      play  \n",
      "1                   run-ing                       run  \n",
      "2                  write-er                     write  \n",
      "3                happy-ness                     happy  \n"
     ]
    }
   ],
   "source": [
    "#nlp 2\n",
    "#morphological\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "data={\n",
    "    'base_word':['play','run','write','happy'],\n",
    "    'added_morph':['-ed','-ing','-er','-ness'],\n",
    "    'new_word':['played','running','writer','happiness'],\n",
    "    'deleted_morph':['-','-','-','-'],\n",
    "    'new_word(after_deletion)':['play','run','write','happy']\n",
    "}\n",
    "\n",
    "df= pd.DataFrame(data)\n",
    "print(df)\n",
    "\n",
    "\n",
    "def add_morph(row):\n",
    "    return row['base_word'] + row['added_morph']\n",
    "\n",
    "def delete_morph(row):\n",
    "    if row['deleted_morph'] != '-':\n",
    "        return row['base_word'].replace(row['deleted_morph'],'')\n",
    "    return row['base_word']\n",
    "\n",
    "df[\"New Word [After Addition]\"]= df.apply(add_morph, axis=1)\n",
    "df[\"New Word [After Deletion]\"]= df.apply(delete_morph, axis=1)\n",
    "    \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b719b3bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#nlp 3\n",
    "#pos\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "\n",
    "from nltk import word_tokenize,pos_tag\n",
    "\n",
    "sentence= \"The quick brown fox jumps over the lazy dog.\"\n",
    "tokens= word_tokenize(sentence)\n",
    "pos= pos_tag(tokens)\n",
    "\n",
    "print(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6282f20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: Barack Obama, Label: PERSON\n",
      "Entity: 44th, Label: ORDINAL\n",
      "Entity: United States, Label: GPE\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Barack Obama\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       " was the \n",
       "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    44th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORDINAL</span>\n",
       "</mark>\n",
       " President of \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    United States\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#nlp 7\n",
    "#ner\n",
    "import spacy\n",
    "\n",
    "nlp= spacy.load(\"en_core_web_sm\")\n",
    "sentence= \"Barack Obama was the 44th President of United States.\"\n",
    "doc= nlp(sentence)\n",
    "\n",
    "for entity in doc.ents:\n",
    "    print(f\"Entity: {entity.text}, Label: {entity.label_}\")\n",
    "\n",
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d4251b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example:\n",
      "Apple:ORG\n",
      "Tim Cook:PERSON\n",
      "California:GPE\n",
      "yesterday:DATE\n",
      "\n",
      "Evaluation report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         GPE       1.00      1.00      1.00         1\n",
      "         ORG       1.00      1.00      1.00         1\n",
      "\n",
      "    accuracy                           1.00         2\n",
      "   macro avg       1.00      1.00      1.00         2\n",
      "weighted avg       1.00      1.00      1.00         2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#nlp 6\n",
    "#build n evaluate ner\n",
    "\n",
    "import spacy\n",
    "from sklearn.metrics import  classification_report\n",
    "\n",
    "def predict_entities(text):\n",
    "    nlp= spacy.load(\"en_core_web_sm\")\n",
    "    doc= nlp(text)\n",
    "    return [(ent.text,ent.label_) for ent in doc.ents]\n",
    "\n",
    "def evaluate_ner(texts_and_labels):\n",
    "    true_labels=[]\n",
    "    pred_labels=[]\n",
    "\n",
    "    for text, true_ents in texts_and_labels:\n",
    "        pred= predict_entities(text)\n",
    "        true_labels.extend([label for _, label in true_ents])\n",
    "        pred_labels.extend([label for _, label in pred])\n",
    "\n",
    "    return classification_report(true_labels,pred_labels)\n",
    "\n",
    "text= \"Apple CEO Tim Cook announced new iphone model in California yesterday.\"\n",
    "print(\"\\nExample:\")\n",
    "for entity, label in predict_entities(text):\n",
    "    print(f\"{entity}:{label}\")\n",
    "\n",
    "test_data=[\n",
    "    (\"Google opened a new office in Paris.\",\n",
    "     [(\"Google\",\"ORG\"), (\"Paris\",\"GPE\")])\n",
    "]\n",
    "\n",
    "print(\"\\nEvaluation report:\")\n",
    "print(evaluate_ner(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229ff1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word 1: dog\n",
      "Word 2: cat\n",
      "Synset1 definition: a member of the genus Canis (probably descended from the common wolf) that has been domesticated by man since prehistoric times; occurs in many breeds\n",
      "Synset2 definition: feline mammal usually having thick soft fur and no ability to roar: domestic cats; wildcats\n",
      "Similarity: 0.8571428571428571\n",
      "Synset1 hypernyms: [Synset('canine.n.02'), Synset('domestic_animal.n.01')]\n",
      "Synset2 hypernyms: [Synset('feline.n.01')]\n",
      "Synset1 hyponyms: [Synset('great_pyrenees.n.01'), Synset('working_dog.n.01'), Synset('hunting_dog.n.01'), Synset('poodle.n.01'), Synset('mexican_hairless.n.01'), Synset('puppy.n.01'), Synset('leonberg.n.01'), Synset('newfoundland.n.01'), Synset('corgi.n.01'), Synset('dalmatian.n.02'), Synset('cur.n.01'), Synset('lapdog.n.01'), Synset('pooch.n.01'), Synset('pug.n.01'), Synset('griffon.n.02'), Synset('spitz.n.01'), Synset('toy_dog.n.01'), Synset('basenji.n.01')]\n",
      "Synset2 hyponyms: [Synset('wildcat.n.03'), Synset('domestic_cat.n.01')]\n"
     ]
    }
   ],
   "source": [
    "#nlp 4\n",
    "#semantic relationship\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get(word1,word2):\n",
    "    synsets1=wn.synsets(word1)\n",
    "    synsets2=wn.synsets(word2)\n",
    "\n",
    "    if not synsets1 or not synsets2:\n",
    "        print(f\"Semantic relationship cannot be determined for {word1} and {word2}\")\n",
    "\n",
    "    synset1= synsets1[0]\n",
    "    synset2= synsets2[0]\n",
    "\n",
    "    relationship={\n",
    "        \"Word 1\":word1,\n",
    "        \"Word 2\":word2,\n",
    "        \"Synset1 definition\":synset1.definition(),\n",
    "        \"Synset2 definition\":synset2.definition(),\n",
    "        \"Similarity\":synset1.wup_similarity(synset2),\n",
    "        \"Synset1 hypernyms\":synset1.hypernyms(),\n",
    "        \"Synset2 hypernyms\":synset2.hypernyms(),\n",
    "        \"Synset1 hyponyms\":synset1.hyponyms(),\n",
    "        \"Synset2 hyponyms\":synset2.hyponyms(),\n",
    "    }\n",
    "    return relationship\n",
    "\n",
    "word1= \"dog\"\n",
    "word2= \"cat\"\n",
    "\n",
    "put= get(word1,word2)\n",
    "\n",
    "for key,value in put.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94f2fb6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next Word: of\n"
     ]
    }
   ],
   "source": [
    "#nlp 5\n",
    "#n gram model\n",
    "\n",
    "import nltk\n",
    "from nltk import trigrams\n",
    "from nltk.corpus import reuters\n",
    "from collections import defaultdict\n",
    "\n",
    "nltk.download('reuters')\n",
    "nltk.download('punkt')\n",
    "\n",
    "words = nltk.word_tokenize(' '.join(reuters.words()))\n",
    "\n",
    "tri_grams = list(trigrams(words))\n",
    "\n",
    "model = defaultdict(lambda: defaultdict(lambda: 0))\n",
    "\n",
    "for w1, w2, w3 in tri_grams:\n",
    " model[(w1, w2)][w3] += 1\n",
    "\n",
    "for w1_w2 in model:\n",
    "    total_count = float(sum(model[w1_w2].values()))\n",
    "    for w3 in model[w1_w2]:\n",
    "        model[w1_w2][w3] /= total_count\n",
    "\n",
    "def predict_next_word(w1, w2):\n",
    "    next_word = model[w1, w2]\n",
    "    if next_word:\n",
    "        predicted_word = max(next_word, key=next_word.get) # Choose the most \n",
    "        return predicted_word\n",
    "    else:\n",
    "        return \"No prediction available\"\n",
    "\n",
    "print(\"Next Word:\", predict_next_word('the', 'stock'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a36afed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
